simulation:
  backend: pybullet  # pybullet | robosuite | gym
  timestep: 0.0041666667
  gravity: -9.81
  gui: true
  max_steps_per_episode: 1000  # Longer episodes for ironing tasks
  
  # Dual robotic arms configuration
  robots:
    left_arm:
      urdf: kuka_iiwa/model.urdf
      base_position: [-0.3, 0.0, 0.0]
      base_orientation_euler: [0.0, 0.0, 0.0]
      joint_limits: [[-3.14, 3.14], [-3.14, 3.14], [-3.14, 3.14], [-3.14, 3.14], [-3.14, 3.14], [-3.14, 3.14], [-3.14, 3.14]]
    right_arm:
      urdf: kuka_iiwa/model.urdf
      base_position: [0.3, 0.0, 0.0]
      base_orientation_euler: [0.0, 0.0, 0.0]
      joint_limits: [[-3.14, 3.14], [-3.14, 3.14], [-3.14, 3.14], [-3.14, 3.14], [-3.14, 3.14], [-3.14, 3.14], [-3.14, 3.14]]
  
  # Linear actuator for heating pad
  linear_actuator:
    enabled: true
    position: [0.0, 0.0, 0.1]  # Above the fabric
    range: [0.0, 0.2]  # Vertical movement range
    speed: 0.01  # m/s
    force: 50.0  # N
  
  # Ironing table and fabric
  workspace:
    table_size: [0.8, 0.6, 0.05]  # Length, width, height
    table_position: [0.0, 0.0, 0.0]
    fabric_size: [0.6, 0.4, 0.001]  # Thickness in meters
    fabric_position: [0.0, 0.0, 0.025]
    fabric_material: "cloth"  # For realistic physics
  
  # Camera configuration for visual feedback
  cameras:
    overhead:
      enabled: true
      position: [0.0, 0.0, 1.0]
      orientation_euler: [0.0, 0.0, 0.0]
      resolution: [640, 480]
      fov: 60
    side_view:
      enabled: true
      position: [0.5, 0.0, 0.3]
      orientation_euler: [0.0, -45.0, 0.0]
      resolution: [640, 480]
      fov: 60


gym:
  entry_point: gymnasium.make
  env_kwargs:
    id: Pendulum-v1
    render_mode: human
  obs_key: null

model:
  architecture: transformer  # Better for sequential multi-agent coordination
  input_dim: 512  # Visual features + proprioceptive state
  action_dim: 15  # 7 joints per arm + 1 linear actuator = 15 total
  hidden_sizes: [512, 512, 256]
  learning_rate: 0.0001  # Lower for stable training
  weight_decay: 1e-4
  gamma: 0.99
  seed: 42
  
  # Multi-agent coordination
  num_agents: 3  # left_arm, right_arm, linear_actuator
  agent_action_dims: [7, 7, 1]  # Actions per agent
  coordination_attention: true
  attention_heads: 8
  attention_dim: 64
  
  # Visual processing
  visual_encoder:
    backbone: "resnet18"
    pretrained: true
    feature_dim: 256
    spatial_resolution: [64, 64]
  
  # Behavior cloning specific
  behavior_cloning:
    enabled: true
    expert_data_path: "./data/expert_demonstrations"
    data_augmentation: true
    temporal_window: 5  # Use 5 timesteps of history
    action_smoothing: true
    smoothing_factor: 0.1

gepa:
  llm_provider: mock
  population_size: 8
  pareto_front_k: 4
  mutation_rate: 0.3
  reflection_weight: 0.5
  evaluation_episodes: 1
  max_iterations: 5
  base_prompt: "Control the robot arm to reach the target smoothly."

experiment:
  project_name: gepa_behavior
  log_dir: ./runs
  checkpoint_dir: ./checkpoints
  seed: 42
  episodes: 3
  steps_per_episode: 200

world_model:
  enabled: true
  architecture: "transformer"  # Better for sequential prediction
  hidden_dim: 512
  learning_rate: 0.0005
  weight_decay: 1e-4
  rollout_horizon: 10  # Longer horizon for fabric dynamics
  train_steps_per_episode: 200
  batch_size: 128
  
  # Fabric physics modeling
  fabric_physics:
    enabled: true
    mesh_resolution: 32  # 32x32 grid for fabric deformation
    material_properties:
      stiffness: 1000.0
      damping: 0.1
      friction: 0.3
    deformation_threshold: 0.01  # Minimum deformation to track
  
  # Multi-agent state prediction
  state_components:
    robot_states: true  # Joint positions, velocities
    fabric_state: true  # Deformation, wrinkles
    contact_forces: true  # Forces between agents and fabric
    visual_features: true  # High-level visual features
  
  # Training configuration
  prediction_loss_weights:
    state_prediction: 1.0
    visual_prediction: 0.5
    contact_prediction: 0.3
    fabric_deformation: 0.8
  
  # Data augmentation for robustness
  augmentation:
    noise_std: 0.01
    temporal_jitter: true
    visual_augmentation: true

# Domain randomization for sim-to-real transfer
domain_randomization:
  enabled: true
  lighting:
    intensity_range: [0.5, 1.5]
    color_temperature_range: [3000, 7000]
    shadow_softness: [0.1, 0.8]
  
  materials:
    fabric_friction_range: [0.2, 0.6]
    fabric_stiffness_range: [500, 2000]
    table_friction_range: [0.3, 0.8]
  
  physics:
    gravity_noise: 0.05
    timestep_noise: 0.02
    joint_damping_noise: 0.1
  
  visual:
    camera_noise: 0.01
    texture_variation: true
    background_variation: true

# Deployment configuration for Jetson Nano
deployment:
  device: "jetson_nano"
  real_time_camera:
    enabled: true
    resolution: [640, 480]
    fps: 30
    camera_id: 0
  
  inference:
    batch_size: 1
    use_tensorrt: true
    precision: "fp16"
    max_workspace_size: 1 << 30  # 1GB
  
  control_loop:
    frequency: 10  # Hz
    action_smoothing: true
    safety_limits: true
    emergency_stop: true

## Examples only (commented out); uncomment to try Gymnasium instead
# gym:
#   entry_point: gymnasium.make
#   env_kwargs:
#     id: Pendulum-v1
#   obs_key: null
